{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2216da21-c353-4c40-ab65-e59f7a0a6e97",
   "metadata": {},
   "source": [
    "# Setup new detection with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0a052-439e-4af7-972e-0349d9ea4315",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80e60332-fee7-4a77-a49f-c20a0d5d4cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import glob\n",
    "from shutil import copy2\n",
    "import os\n",
    "\n",
    "import pathlib as pl\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1de84747-1016-4f84-9230-f39ea0194b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SIZE_FACTOR = 0.3\n",
    "CRITICAL_DIST_FROM_OTHER_CENTER = 20\n",
    "TARGET_SIZE = (24, 68)\n",
    "AREA_BORDER = TARGET_SIZE[0] * TARGET_SIZE[1]\n",
    "WIDTH_TO_HEIGHT = TARGET_SIZE[0] / TARGET_SIZE[1]\n",
    "MIN_WIDTH_TO_HEIGHT = WIDTH_TO_HEIGHT * (1 - SIZE_FACTOR)\n",
    "MAX_WIDTH_TO_HEIGHT = WIDTH_TO_HEIGHT * (1 + SIZE_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39d831ab-c812-451a-ba6a-a8b883427390",
   "metadata": {},
   "outputs": [],
   "source": [
    "RED = (0, 0, 255)\n",
    "BLUE = (255, 0, 0)\n",
    "PURPLE = (255, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31fb9668-d3a1-4500-87ee-9a29e22f17f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_rotation(min_rect):\n",
    "    \"\"\"\n",
    "    Prepare portrait rotation. No difference between up and down.\n",
    "    \"\"\"\n",
    "    width_to_height = min_rect[1][0] / min_rect[1][1]\n",
    "    \n",
    "    if width_to_height >= 1:\n",
    "        return 90 - min_rect[2]\n",
    "    else:\n",
    "        return min_rect[2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6790dc-d62a-4915-8634-6983de2b7db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rotate_and_crop_min_rect(image, min_area_rect):\n",
    "    factor = 1.3\n",
    "\n",
    "    width_to_height = min_area_rect[1][0] / min_area_rect[1][1]\n",
    "    if width_to_height >= 1:\n",
    "        angle = -1 * (90 - min_area_rect[2])\n",
    "    else:\n",
    "        angle = min_area_rect[2]    \n",
    "\n",
    "    size_of_transformed_image = max(min_area_rect[1])\n",
    "    min_needed_height = int(np.sqrt(2 * np.power(size_of_transformed_image, 2)))\n",
    "    size = (min_needed_height, min_needed_height)\n",
    "\n",
    "    box = cv2.boxPoints(min_area_rect)\n",
    "    box = np.intp(box)\n",
    "\n",
    "    x_coordinates_of_box = box[:,0]\n",
    "    y_coordinates_of_box = box[:,1]\n",
    "    x_min = min(x_coordinates_of_box)\n",
    "    x_max = max(x_coordinates_of_box)\n",
    "    y_min = min(y_coordinates_of_box)\n",
    "    y_max = max(y_coordinates_of_box)\n",
    "\n",
    "    center = (int((x_min+x_max)/2), int((y_min+y_max)/2))\n",
    "    cropped = cv2.getRectSubPix(image, size, center)\n",
    "    M = cv2.getRotationMatrix2D((size[0]/2, size[1]/2), angle, 1.0)\n",
    "    cropped = cv2.warpAffine(cropped, M, size)\n",
    "\n",
    "    width = round(min_area_rect[1][0])\n",
    "    height = round(min_area_rect[1][1])\n",
    "\n",
    "    if width_to_height >= 1:\n",
    "        cropped_rotated = cv2.getRectSubPix(cropped, (int(factor * height), int(factor * width)), (size[0]/2, size[1]/2))\n",
    "    else:\n",
    "        cropped_rotated = cv2.getRectSubPix(cropped, (int(factor * width), int(factor * height)), (size[0]/2, size[1]/2))\n",
    "\n",
    "    return cropped_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd809ce-8144-4b38-8b69-a726af6129bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_contours_manually(cnts, frame, color):\n",
    "    for num, contour in enumerate(cnts):\n",
    "        con_frame = frame.copy()\n",
    "        cv2.drawContours(con_frame, cnts, num, color, thickness=10)\n",
    "        cv2.imshow('Frame',con_frame)\n",
    "        key = cv2.waitKey(0) & 0xFF\n",
    "        if key == ord('y'):\n",
    "            return contour\n",
    "        elif key == ord('q') or key == 27:\n",
    "            return None\n",
    "        elif key == ord('r'):\n",
    "            return check_contours_manually(cnts, frame, color)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4cfd850-e977-490c-9368-6ee2c1ef1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_extract_img_from_cnt(gray_img, con):\n",
    "    min_rect = cv2.minAreaRect(con)\n",
    "    center, size, angle = min_rect\n",
    "    area = size[0] * size[1]\n",
    "\n",
    "    if area < AREA_BORDER:\n",
    "        return None\n",
    "\n",
    "    low_value = min(size[0], size[1])\n",
    "    high_value = max(size[0], size[1])\n",
    "    width_to_height = low_value / high_value\n",
    "\n",
    "    if MIN_WIDTH_TO_HEIGHT < width_to_height < MAX_WIDTH_TO_HEIGHT:\n",
    "        cropped_img = rotate_and_crop_min_rect(gray_img, min_rect)\n",
    "        small_img = cv2.resize(cropped_img, TARGET_SIZE)\n",
    "        return small_img\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b16e4b3-39f6-4129-b401-77bcee61bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_extract_norm_img_from_cnt(gray_img, con):\n",
    "    small_img = filter_and_extract_img_from_cnt(gray_img, con)\n",
    "    if small_img is not None:\n",
    "        small_img = small_img / 255\n",
    "        return small_img\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00806bed-2bf0-462d-8310-29ae2f83395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img_from_cnt(gray_img, cnt):\n",
    "    min_rect = cv2.minAreaRect(cnt)\n",
    "    cropped_img = rotate_and_crop_min_rect(gray_img, min_rect)\n",
    "    small_img = cv2.resize(cropped_img, TARGET_SIZE)\n",
    "    return small_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85f65cf9-6e18-4977-b3fa-1fc2d9a4ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_time_string():\n",
    "    return datetime.now(timezone.utc).strftime('%Y-%m-%d_%H-%M-%S-%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "489dc05c-dff8-4672-a40b-2fac5794c347",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_path = '../../2023-09-01-163208.webm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "333f1244-c2c6-4b51-b8d6-1c6adf8cf1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '../../2023-12-04-201531.webm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802b8a6-a0df-46aa-9726-3d33ad8c5db4",
   "metadata": {},
   "source": [
    "## extract positive training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d871c7-c340-4e73-b05c-7435ed4ca5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model...\n",
      "model not yet created\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     29\u001b[0m     frameId \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "\n",
    "import pdb\n",
    "import traceback\n",
    "\n",
    "if not 'model' in dir():\n",
    "    print('load model...')\n",
    "    from keras.models import load_model\n",
    "    model_filename = 'arrow_detection.h5'\n",
    "    if pl.Path(model_filename).exists():\n",
    "        print('loaded model')\n",
    "    else:\n",
    "        print('model not yet created')\n",
    "\n",
    "if 'model' in dir():\n",
    "    model.trainable = False\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    cap.release()\n",
    "    raise SystemExit()\n",
    "\n",
    "frameRate = cap.get(5) #frame rate\n",
    "abort = False\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        frameId = cap.get(1)\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            continue\n",
    "            \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "    \n",
    "        key = cv2.waitKey(25) & 0xFF\n",
    "    \n",
    "        # Press Q on keyboard to  exit\n",
    "        if key == ord('q') or abort:\n",
    "            break\n",
    "        elif frameId % np.floor(frameRate/2) == 0:\n",
    "            centers = []\n",
    "\n",
    "            gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            blurred = cv2.blur(gray_img, (3,3))\n",
    "            \n",
    "            sigma = 0.33\n",
    "            v = np.median(blurred)\n",
    "            \n",
    "            #---- apply automatic Canny edge detection using the computed median----\n",
    "            lower = int(max(0, (1.0 - sigma) * v))    #---- lower threshold\n",
    "            upper = int(min(255, (1.0 + sigma) * v))  #---- upper threshold\n",
    "            thresh_img = cv2.Canny(blurred, lower, upper)\n",
    "            cnts, _ = cv2.findContours(thresh_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "            filtered_norm_imgs = [] # TODO: use np.ma next time\n",
    "            filtered_cnts = []\n",
    "            pos_filtered_to_pos_source = {}\n",
    "            pos_filtered = 0\n",
    "            for pos_source, con in enumerate(cnts):\n",
    "                small_img = filter_and_extract_norm_img_from_cnt(blurred, con)\n",
    "                if small_img is not None:\n",
    "                    filtered_cnts.append(con)\n",
    "                    filtered_norm_imgs.append(small_img)\n",
    "                    pos_filtered_to_pos_source[pos_filtered] = pos_source\n",
    "                    pos_filtered += 1\n",
    "            \n",
    "            filtered_norm_imgs = np.array(filtered_norm_imgs)\n",
    "            if 'model' in dir():\n",
    "                prediction = model.predict(filtered_norm_imgs).flatten()\n",
    "            else:\n",
    "                prediction = [False] * len(filtered_norm_imgs)\n",
    "\n",
    "            positive_contours = []\n",
    "            negative_contours = []\n",
    "\n",
    "            for pos, value in enumerate(prediction):\n",
    "                idx = pos_filtered_to_pos_source[pos]\n",
    "                if value >= 0.5:\n",
    "                    M = cv2.moments(cnts[idx])\n",
    "                    center_x = M['m10'] // M['m00']\n",
    "                    center_y = M['m01'] // M['m00']\n",
    "                    #TODO: filter based on centers\n",
    "                    too_close_to_other_center = False\n",
    "                    for x, y in centers:\n",
    "                        if x-CRITICAL_DIST_FROM_OTHER_CENTER < center_x < x+CRITICAL_DIST_FROM_OTHER_CENTER:\n",
    "                            if y-CRITICAL_DIST_FROM_OTHER_CENTER < center_y < y+CRITICAL_DIST_FROM_OTHER_CENTER:\n",
    "                                too_close_to_other_center = True\n",
    "                                print(f'{center_x} and {center_y} are too close to another center')\n",
    "                                break\n",
    "\n",
    "                    if not too_close_to_other_center:\n",
    "                        centers.append((center_x, center_y))\n",
    "                        positive_contours.append(cnts[idx])\n",
    "                else:\n",
    "                    negative_contours.append(cnts[idx])\n",
    "\n",
    "            con_frame = frame.copy()\n",
    "            \n",
    "            if len(positive_contours):\n",
    "                cv2.drawContours(con_frame, positive_contours, -1, BLUE, thickness=10)\n",
    "            if len(negative_contours):\n",
    "                cv2.drawContours(con_frame, negative_contours, -1, RED, thickness=10)\n",
    "                \n",
    "            if len(positive_contours) > 1:\n",
    "                cv2.putText(con_frame, f'many good values found; len(positive_contours)={len(positive_contours)}', (10,460), cv2.FONT_HERSHEY_SIMPLEX, .5, RED)    \n",
    "            elif len(positive_contours) == 1:\n",
    "                cv2.putText(con_frame, f'one good values found; len(positive_contours)={len(positive_contours)}', (10,460), cv2.FONT_HERSHEY_SIMPLEX, .5, BLUE)    \n",
    "            else:\n",
    "                cv2.putText(con_frame, f'no good value found; len(filtered_norm_imgs)={len(filtered_norm_imgs)}', (10,460), cv2.FONT_HERSHEY_SIMPLEX, .5, RED)\n",
    "                \n",
    "            cv2.imshow('Frame',con_frame)\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            \n",
    "            if key == ord('n'):\n",
    "                contour  = check_contours_manually(filtered_cnts, frame, PURPLE)\n",
    "                if contour is not None:\n",
    "                    small_img = extract_img_from_cnt(blurred, contour)\n",
    "                    timestring = datetime.now(timezone.utc).strftime('%Y-%m-%d_%H-%M-%S-%f')\n",
    "                    retval = cv2.imwrite(f'../original-positives/img_{timestring}.jpg', small_img)\n",
    "                    if not retval:\n",
    "                        raise ValueError(f'retval:{retval}')\n",
    "      \n",
    "            elif key == ord('y'):\n",
    "                if len(positive_contours) == 1:\n",
    "                    small_img = extract_img_from_cnt(blurred, positive_contours[0])\n",
    "                    timestring = get_current_time_string()\n",
    "                    retval = cv2.imwrite(f'../original-positives/img_{timestring}.jpg', small_img)\n",
    "                    if not retval:\n",
    "                        raise ValueError(f'retval:{retval}')\n",
    "                \n",
    "                else:\n",
    "                    print('too many values -> no save')\n",
    "                \n",
    "            elif key == ord('q') or key == 27:\n",
    "                abort = True\n",
    "                break\n",
    "\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    #print(e)\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4652c94c-4ad4-4452-9650-e520a5fd6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c77ee-a702-4745-89a4-60f291b3259d",
   "metadata": {},
   "source": [
    "## Create multiple images of pos images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d1a12c-3784-4550-a42f-687ffb8a6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_versions = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4ac1ecd-9b35-4152-9b68-78bddcbd452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator(rotation_range=2, rescale=1. / 255, shear_range=0.05,\n",
    "                              zoom_range=0.01, horizontal_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e93d1ced-7b77-4b41-9e38-b49679e14632",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pl.Path('../original-positives/')\n",
    "\n",
    "if not path.is_dir():\n",
    "    raise IOError('path not valid')\n",
    "    \n",
    "for num, image_name in enumerate(path.iterdir()):\n",
    "    # Create image to tensor\n",
    "    img = load_img(str(image_name), grayscale=False)\n",
    "    arr = img_to_array(img)\n",
    "    tensor_image = arr.reshape((1, ) + arr.shape)\n",
    "\n",
    "    for nbr_of_created_imgs, _ in enumerate(data_gen.flow(\n",
    "        x=tensor_image,\n",
    "        batch_size=1,\n",
    "        save_to_dir='../pos',\n",
    "        save_prefix='generated',\n",
    "        save_format='jpg')\n",
    "    ):\n",
    "        if nbr_of_created_imgs >= generated_versions:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5d995-56f3-41c4-9c0b-255d10660aa7",
   "metadata": {},
   "source": [
    "## Extract negative training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d9e0b6-8d5d-426a-ae67-6a5d283eea16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neg_image_path = '../../raw-images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6630a01-378c-40af-a3f4-fdb9b2ed776c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pl.Path(neg_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44e523e6-2532-48d8-ae32-8f7b95b53fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at least 70 raw images finished\r"
     ]
    }
   ],
   "source": [
    "for num, image_name in enumerate(path.iterdir()):\n",
    "    big_neg_image = cv2.imread(str(image_name))\n",
    "    if big_neg_image is None:\n",
    "        print(f'could not load image {image_name}')\n",
    "        break\n",
    "    \n",
    "    blurred = cv2.blur(big_neg_image, (3,3))\n",
    "    gray_img = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)\n",
    "    #thresh_img = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 4)\n",
    "    #thresh_img = cv2.Canny(gray_img, 100, 255)\n",
    "    \n",
    "    sigma = 0.33\n",
    "    v = np.median(gray_img)\n",
    "\n",
    "    #---- apply automatic Canny edge detection using the computed median----\n",
    "    lower = int(max(0, (1.0 - sigma) * v))    #---- lower threshold\n",
    "    upper = int(min(255, (1.0 + sigma) * v))  #---- upper threshold\n",
    "    thresh_img = cv2.Canny(gray_img, lower, upper)\n",
    "    #plt.imshow(thresh_img)\n",
    "    cnts, _ = cv2.findContours(thresh_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for con in cnts:\n",
    "        small_img = filter_and_extract_img_from_cnt(gray_img, con)\n",
    "        if small_img is not None:\n",
    "            timestring = get_current_time_string()\n",
    "            retval = cv2.imwrite(f'../../unused-neg/img_{timestring}.jpg', small_img)\n",
    "            if not retval:\n",
    "                raise ValueError(f'retval saving small img:{retval}')\n",
    "                \n",
    "    if num % 10 == 0:\n",
    "        print(f'at least {num} raw images finished', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc9e29-4490-4445-8286-7709336896e9",
   "metadata": {},
   "source": [
    "## Move random negative files to neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7aa4f9aa-14a5-46cb-a87d-50c7da27f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_neg_filenames = random.sample(glob.glob('../../unused-neg/*.jpg'), k=10_000)\n",
    "for filename in used_neg_filenames:\n",
    "    copy2(filename, f'../neg/{os.path.basename(filename)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3530b-4045-4b18-a5c6-87651e335324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
